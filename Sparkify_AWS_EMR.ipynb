{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn predictive modelling using Apache Spark (PySpark) with Sparkify dataset\n",
    "\n",
    "This project sets to create a predictive model for churn prediction of a music streaming service: Sparkify. \n",
    "\n",
    "Two dataset are made available, a tiny set of 128Mb and a full dataset of 12Gb. \n",
    "The project will train the tiny dataset on a local machine to get a sense of the sample data before deciding the components necessary to model on the full dataset. \n",
    "\n",
    "Aside from data ecxploration, the local modelling work will find out how to preprocess the data, what features to select and the suitable learning algorithm to adopt. Doing so will make the modelling work more time and computationally efficient. \n",
    "\n",
    "For modelling work on the large dataset, AWS EMR cluster will be adopted to do the final training and modelling work. We will also compare to see if full dataset behaves simialrly as well as descriptively similar to the tiny dataset. As such our choice for training features and learning algorithm are wise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1566628057740_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-25-27.us-west-2.compute.internal:20888/proxy/application_1566628057740_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-21-7.us-west-2.compute.internal:8042/node/containerlogs/container_1566628057740_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "# Starter code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, col, count, desc, stddev, udf, isnan, when, isnull, mean, min, max\n",
    "from pyspark.sql.types import IntegerType, BooleanType\n",
    "from pyspark.sql.functions import max as max_fn\n",
    "from pyspark.sql.functions import min as min_fn\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "\n",
    "The full dataset 12Gb is loaded from an AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist=u'Popol Vuh', auth=u'Logged In', firstName=u'Shlok', gender=u'M', itemInSession=278, lastName=u'Johnson', length=524.32934, level=u'paid', location=u'Dallas-Fort Worth-Arlington, TX', method=u'PUT', page=u'NextSong', registration=1533734541000, sessionId=22683, song=u'Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent=u'\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=u'1749042')"
     ]
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3a://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of data points in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26259199"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A brief description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+---------+--------+------------------+--------+------------------+--------+--------------+--------+--------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|summary|            artist|      auth|firstName|  gender|     itemInSession|lastName|            length|   level|      location|  method|    page|        registration|         sessionId|                song|            status|                  ts|           userAgent|            userId|\n",
      "+-------+------------------+----------+---------+--------+------------------+--------+------------------+--------+--------------+--------+--------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|  count|          20850272|  26259199| 25480720|25480720|          26259199|25480720|          20850272|26259199|      25480720|26259199|26259199|            25480720|          26259199|            20850272|          26259199|            26259199|            25480720|          26259199|\n",
      "|   mean|511.61795374767706|      null| Infinity|    null|106.56267561702853|    null|248.72543296748836|    null|          null|    null|    null|1.535220665260513...|100577.99253503505|            Infinity|210.06768953615074|1.540905636113772...|                null|1488379.8347142653|\n",
      "| stddev| 968.6388398781419|      null|      NaN|    null|  117.658126175238|    null| 97.28710387078068|    null|          null|    null|    null|3.2402990978250675E9| 71909.21077875949|                 NaN|31.550728788197635|1.5158105552717402E9|                null|286970.08894623973|\n",
      "|    min|               !!!| Cancelled|    Aaden|       F|                 0|  Abbott|             0.522|    free|  Aberdeen, SD|     GET|   About|       1508018725000|                 1|  \u0018Till Kingdom Come|               200|       1538352001000|\"Mozilla/5.0 (Mac...|           1000025|\n",
      "|    max|            ÃÂ¼NN|Logged Out|Zytavious|       M|              1428|  Zuniga|        3024.66567|    paid|Zanesville, OH|     PUT| Upgrade|       1543821822000|            240381|ÃÂ¾etta Gerist Ã...|               404|       1543622402000|Mozilla/5.0 (comp...|           1999996|\n",
      "+-------+------------------+----------+---------+--------+------------------+--------+------------------+--------+--------------+--------+--------+--------------------+------------------+--------------------+------------------+--------------------+--------------------+------------------+"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "|           Downgrade|\n",
      "|               Error|\n",
      "|                Help|\n",
      "|                Home|\n",
      "|               Login|\n",
      "|              Logout|\n",
      "|            NextSong|\n",
      "|            Register|\n",
      "|         Roll Advert|\n",
      "|       Save Settings|\n",
      "|            Settings|\n",
      "|    Submit Downgrade|\n",
      "| Submit Registration|\n",
      "|      Submit Upgrade|\n",
      "|         Thumbs Down|\n",
      "|           Thumbs Up|\n",
      "|             Upgrade|\n",
      "+--------------------+"
     ]
    }
   ],
   "source": [
    "df.select(\"page\").distinct().sort(\"page\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist: 5408927\n",
      "firstName: 778479\n",
      "gender: 778479\n",
      "lastName: 778479\n",
      "length: 5408927\n",
      "location: 778479\n",
      "registration: 778479\n",
      "song: 5408927\n",
      "userAgent: 778479"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    missing_count = df.filter((isnan(df[col])) | (df[col].isNull()) | (df[col] == \"\")).count()\n",
    "    if missing_count > 0:\n",
    "        print(\"{}: {}\".format(col, missing_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with missing values in userId and sessionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Pyspark dataframe: 26259199"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the Pyspark dataframe: {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "df_cleaned = df_cleaned.filter(df[\"userId\"] != \"\") # `userId` should not be empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after clearning: 26259199"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows after clearning: {}\".format(df_cleaned.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no missing values in userId and sessionId"
     ]
    }
   ],
   "source": [
    "if df.count() == df_cleaned.count():\n",
    "    print(\"There is no missing values in userId and sessionId\")\n",
    "else:\n",
    "    print(\"{} rows removed.\".format(df.count() - df_cleaned.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = []\n",
    "categorical_cols = []\n",
    "\n",
    "for s in df_cleaned.schema:\n",
    "    data_type = str(s.dataType)\n",
    "    if data_type == \"StringType\":\n",
    "        categorical_cols.append(s.name)\n",
    "    \n",
    "    if data_type == \"LongType\" or data_type == \"DoubleType\":\n",
    "        numerical_cols.append(s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist count: 26259199\n",
      "auth count: 26259199\n",
      "firstName count: 26259199\n",
      "gender count: 26259199\n",
      "lastName count: 26259199\n",
      "level count: 26259199\n",
      "location count: 26259199\n",
      "method count: 26259199\n",
      "page count: 26259199\n",
      "song count: 26259199\n",
      "userAgent count: 26259199\n",
      "userId count: 26259199"
     ]
    }
   ],
   "source": [
    "for c in categorical_cols: \n",
    "    print(\"{} count: {}\".format(c, df_cleaned.select(c).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemInSession count: 26259199\n",
      "+------------------+------------------+------------------+--------------------------+\n",
      "|avg(itemInSession)|min(itemInSession)|max(itemInSession)|stddev_samp(itemInSession)|\n",
      "+------------------+------------------+------------------+--------------------------+\n",
      "|106.56267561702853|                 0|              1428|        117.65812617523798|\n",
      "+------------------+------------------+------------------+--------------------------+\n",
      "\n",
      "length count: 26259199\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "|       avg(length)|min(length)|max(length)|stddev_samp(length)|\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "|248.72543296748836|      0.522| 3024.66567|  97.28710387078071|\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "\n",
      "registration count: 26259199\n",
      "+--------------------+-----------------+-----------------+-------------------------+\n",
      "|   avg(registration)|min(registration)|max(registration)|stddev_samp(registration)|\n",
      "+--------------------+-----------------+-----------------+-------------------------+\n",
      "|1.535220665260512...|    1508018725000|    1543821822000|     3.2402990978250685E9|\n",
      "+--------------------+-----------------+-----------------+-------------------------+\n",
      "\n",
      "sessionId count: 26259199\n",
      "+------------------+--------------+--------------+----------------------+\n",
      "|    avg(sessionId)|min(sessionId)|max(sessionId)|stddev_samp(sessionId)|\n",
      "+------------------+--------------+--------------+----------------------+\n",
      "|100577.99253503505|             1|        240381|     71909.21077875949|\n",
      "+------------------+--------------+--------------+----------------------+\n",
      "\n",
      "status count: 26259199\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "|       avg(status)|min(status)|max(status)|stddev_samp(status)|\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "|210.06768953615074|        200|        404| 31.550728788197617|\n",
      "+------------------+-----------+-----------+-------------------+\n",
      "\n",
      "ts count: 26259199\n",
      "+--------------------+-------------+-------------+--------------------+\n",
      "|             avg(ts)|      min(ts)|      max(ts)|     stddev_samp(ts)|\n",
      "+--------------------+-------------+-------------+--------------------+\n",
      "|1.540905636113773E12|1538352001000|1543622402000|1.5158105552719693E9|\n",
      "+--------------------+-------------+-------------+--------------------+"
     ]
    }
   ],
   "source": [
    "for n in numerical_cols: \n",
    "    print(\"{} count: {}\".format(n, df_cleaned.select(n).count()))\n",
    "    df_cleaned.select([mean(n), min(n), max(n), stddev(n)]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38338"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"artist\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "|Logged Out|\n",
      "| Cancelled|\n",
      "|     Guest|\n",
      "| Logged In|\n",
      "+----------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"auth\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"firstName\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|  null|\n",
      "|     M|\n",
      "+------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"itemInSession\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"lastName\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23749"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"length\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"level\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"location\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|       Jonesboro, AR|\n",
      "|     Gainesville, TX|\n",
      "|Iron Mountain, MI-WI|\n",
      "|        Columbus, NE|\n",
      "|          Tucson, AZ|\n",
      "|        Richmond, VA|\n",
      "| Lewiston-Auburn, ME|\n",
      "|Florence-Muscle S...|\n",
      "|       Muscatine, IA|\n",
      "|       Oskaloosa, IA|\n",
      "|          Warsaw, IN|\n",
      "|      Mount Airy, NC|\n",
      "|          Uvalde, TX|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "|Deltona-Daytona B...|\n",
      "|  Clarksville, TN-KY|\n",
      "|         Madison, IN|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"location\").distinct().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"method\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"page\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22248"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"registration\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228713"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"sessionId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253565"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"song\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"status\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userAgent                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                     |\n",
      "|\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                                       |\n",
      "|Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0                                                                     |\n",
      "|Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20100101 Firefox/29.0                                                                       |\n",
      "|\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) GSA/4.1.0.31802 Mobile/11D257 Safari/9537.53\"|\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"                      |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"                     |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\"                     |\n",
      "|Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:31.0) Gecko/20100101 Firefox/31.0                                                              |\n",
      "|Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"userAgent\").distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22278"
     ]
    }
   ],
   "source": [
    "df_cleaned.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of cancellations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.filter(df_cleaned.page==\"Cancellation Confirmation\").select(\"userId\").dropDuplicates().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|churn|gender|count|\n",
      "+-----+------+-----+\n",
      "|false|  null|    1|\n",
      "|false|     F| 8279|\n",
      "|false|     M| 8995|\n",
      "| true|     F| 2347|\n",
      "| true|     M| 2656|\n",
      "+-----+------+-----+"
     ]
    }
   ],
   "source": [
    "churn_list = df_cleaned.filter(df_cleaned.page==\"Cancellation Confirmation\" ).select(\"userId\").dropDuplicates()\n",
    "churned_users = [(row['userId']) for row in churn_list.collect()]\n",
    "df_churn = df_cleaned.withColumn(\"churn\", df_cleaned.userId.isin(churned_users))\n",
    "df_churn.dropDuplicates([\"userId\", \"gender\"]).groupby([\"churn\", \"gender\"]).count().sort(\"churn\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_events = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_cleaned = df_cleaned.withColumn(\"churn_flag\", churn_events(\"page\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 45650)\n",
      "----------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 655, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of users who churned\n",
    "churn_flag = df_cleaned.groupBy('userId').agg({'churn_flag': 'sum'})\\\n",
    "    .select(avg('sum(churn_flag)')).collect()[0]['avg(sum(churn_flag))']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.457 % of users have churned by cancelling subscription."
     ]
    }
   ],
   "source": [
    "print(\"{} % of users have churned by cancelling subscription.\".format(round(churn_flag*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User churn percentage of 22.457 % is very close to those of the tiny dataset of 22.098%. We can at least assume that label skewness is likely similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.filter(df_cleaned.page==\"Cancellation Confirmation\").select(\"userId\").dropDuplicates().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of downgrades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15209"
     ]
    }
   ],
   "source": [
    "df_cleaned.filter(df_cleaned.page==\"Downgrade\").select(\"userId\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|downgrade|gender|count|\n",
      "+---------+------+-----+\n",
      "|    false|     F| 3371|\n",
      "|    false|  null|    1|\n",
      "|    false|     M| 3697|\n",
      "|     true|     F| 7255|\n",
      "|     true|     M| 7954|\n",
      "+---------+------+-----+"
     ]
    }
   ],
   "source": [
    "downgrade_list = df_cleaned.filter(df_cleaned.page==\"Downgrade\" ).select(\"userId\").distinct()\n",
    "downgraded_users = [(row['userId']) for row in downgrade_list.collect()]\n",
    "df_downgrade = df_cleaned.withColumn(\"downgrade\", df_cleaned.userId.isin(downgraded_users))\n",
    "df_downgrade.dropDuplicates([\"userId\", \"gender\"]).groupby([\"downgrade\", \"gender\"]).count().sort(\"downgrade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gender (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest level\n",
    "fn_gender = udf(lambda x: 1 if x==\"F\" else 0, IntegerType())\n",
    "feat_gender = df_cleaned.select(['userId', 'gender'])\\\n",
    "    .dropDuplicates(['userId'])\\\n",
    "    .select(['userId', 'gender'])\\\n",
    "    .withColumn('gender', fn_gender('gender').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|            userId|             gender|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|             22278|              22278|\n",
      "|   mean|1498782.9615764432|0.47697279827632644|\n",
      "| stddev| 288851.8472659188| 0.4994806768184825|\n",
      "|    min|           1000025|                  0|\n",
      "|    max|           1999996|                  1|\n",
      "+-------+------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "feat_gender.describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paid or Free (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest level\n",
    "fn_level = udf(lambda x: 1 if x==\"paid\" else 0, IntegerType())\n",
    "feat_level = df_cleaned.select(['userId', 'level', 'ts'])\\\n",
    "    .orderBy(desc('ts'))\\\n",
    "    .dropDuplicates(['userId'])\\\n",
    "    .select(['userId', 'level'])\\\n",
    "    .withColumn('level', fn_level('level').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|            userId|              level|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|             22278|              22278|\n",
      "|   mean|1498782.9615764432| 0.5992010054762547|\n",
      "| stddev| 288851.8472659188|0.49007136327332323|\n",
      "|    min|           1000025|                  0|\n",
      "|    max|           1999996|                  1|\n",
      "+-------+------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "feat_level.describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total number of songs listened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userID|          num_song|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             22278|             22278|\n",
      "|   mean|1498782.9615764432|1178.7054044348686|\n",
      "| stddev|288851.84726591856|  5372.95993988227|\n",
      "|    min|           1000025|                 1|\n",
      "|    max|           1999996|            778479|\n",
      "+-------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "feat_song = df_cleaned \\\n",
    "              .select([\"userId\",\"song\"]) \\\n",
    "              .groupby(\"userID\") \\\n",
    "              .count()\\\n",
    "              .withColumnRenamed(\"count\", \"num_song\") \\\n",
    "              .orderBy(\"userId\")\n",
    "\n",
    "feat_song.describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total number of artist listened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|            userId|       num_artist|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             22261|            22261|\n",
      "|   mean|1498833.2082116706|645.0307263824626|\n",
      "| stddev| 288882.1163228876|602.2479741901458|\n",
      "|    min|           1000025|                1|\n",
      "|    max|           1999996|             4368|\n",
      "+-------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "# Number of artists listened\n",
    "feat_artist = df_cleaned \\\n",
    "    .filter(df_cleaned.page==\"NextSong\") \\\n",
    "    .select(\"userId\", \"artist\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupby(\"userId\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"num_artist\") \\\n",
    "    .orderBy(\"userId\")\n",
    "\n",
    "feat_artist.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of songs in playlist(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|            userID|num_playlist_song|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             21260|            21260|\n",
      "|   mean|1498898.9698494826|28.12422389463782|\n",
      "| stddev|289180.40429718536|32.27499039023108|\n",
      "|    min|           1000025|                1|\n",
      "|    max|           1999996|              340|\n",
      "+-------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "feat_playlist = df_cleaned \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_cleaned.page == 'Add to Playlist') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'num_playlist_song') \\\n",
    "    .orderBy(\"userId\")\n",
    "feat_playlist.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userID|        num_friend|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             20305|             20305|\n",
      "|   mean| 1499371.503718296| 18.79655257325782|\n",
      "| stddev|288830.59626148926|20.747704116295065|\n",
      "|    min|           1000025|                 1|\n",
      "|    max|           1999996|               222|\n",
      "+-------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "feat_friends = df_cleaned \\\n",
    "    .select('userID','page') \\\n",
    "    .where(df_cleaned.page == 'Add Friend') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'num_friend') \\\n",
    "    .orderBy(\"userId\")\n",
    "feat_friends.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total length of listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userID|       time_listen|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             22278|             22261|\n",
      "|   mean|1498782.9615764432|232963.16116480672|\n",
      "| stddev| 288851.8472659186|273559.41985437507|\n",
      "|    min|           1000025|          78.49751|\n",
      "|    max|           1999996|     2807182.33115|\n",
      "+-------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# Total length of listening\n",
    "feat_listentime = df_cleaned \\\n",
    "    .select('userID','length') \\\n",
    "    .groupBy('userID') \\\n",
    "    .sum() \\\n",
    "    .withColumnRenamed('sum(length)', 'time_listen') \\\n",
    "    .orderBy(\"userId\")\n",
    "feat_listentime.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average number of songs per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|            userId|        avg_songs|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             22261|            22261|\n",
      "|   mean|1498833.2082116706|67.28930119633611|\n",
      "| stddev| 288882.1163228875|42.00146132153544|\n",
      "|    min|           1000025|              1.0|\n",
      "|    max|           1999996|            579.0|\n",
      "+-------+------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "feat_avgsongs = df_cleaned.filter(df_cleaned.page ==\"NextSong\") \\\n",
    "                               .groupBy([\"userId\", \"sessionId\"]) \\\n",
    "                               .count() \\\n",
    "                               .groupby(['userId']) \\\n",
    "                               .agg({'count':'avg'}) \\\n",
    "                               .withColumnRenamed('avg(count)', 'avg_songs') \\\n",
    "                               .orderBy(\"userId\")\n",
    "\n",
    "feat_avgsongs.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average time per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userId|       avgSessTime|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             22278|             22278|\n",
      "|   mean|1498782.9615764432| 276.5377760334103|\n",
      "| stddev| 288851.8472659185|180.68117321920786|\n",
      "|    min|           1000025|               0.0|\n",
      "|    max|           1999996| 5453.363730301772|\n",
      "+-------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "feat_sesstime = df_cleaned.groupBy([\"userId\", \"sessionId\"]) \\\n",
    "                .agg(((max_fn(df_cleaned.ts)-min_fn(df_cleaned.ts))/(1000*60))\n",
    "                .alias(\"sessTime\"))\n",
    "feat_avgtime = feat_sesstime.groupby(\"userId\") \\\n",
    "                    .agg(avg(feat_sesstime.sessTime).alias(\"avgSessTime\")) \\\n",
    "                    .orderBy(\"userId\")\n",
    "\n",
    "feat_avgtime.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "| userId|       avgSessTime|\n",
      "+-------+------------------+\n",
      "|1000025|  404.793137254902|\n",
      "|1000035| 235.9363636363636|\n",
      "|1000083|186.10454545454547|\n",
      "|1000103| 68.93333333333334|\n",
      "|1000164|218.88981481481483|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "feat_avgtime.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of session per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            userId|           session|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             22278|             22278|\n",
      "|   mean|1498782.9615764432|20.431726366819284|\n",
      "| stddev|288851.84726591856|1059.3297847404108|\n",
      "|    min|           1000025|                 1|\n",
      "|    max|           1999996|            158115|\n",
      "+-------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "feat_session = df_cleaned.select(\"userId\", \"sessionId\") \\\n",
    "                .dropDuplicates() \\\n",
    "                .groupby(\"userId\") \\\n",
    "                .count() \\\n",
    "                .withColumnRenamed('count', 'session') \\\n",
    "                .orderBy(\"userId\")\n",
    "feat_session.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label (churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label user who churned using the churn_flag defined earlier. \n",
    "user_partitions = Window.partitionBy('userId')\n",
    "df_cleaned = df_cleaned.withColumn('churn', max('churn_flag').over(user_partitions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|            userId|              label|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|             22278|              22278|\n",
      "|   mean|1498782.9615764432|0.22457132597181076|\n",
      "| stddev| 288851.8472659188| 0.4173090731235619|\n",
      "|    min|           1000025|                  0|\n",
      "|    max|           1999996|                  1|\n",
      "+-------+------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "label = df_cleaned \\\n",
    "    .select(['userId', 'churn']) \\\n",
    "    .dropDuplicates() \\\n",
    "    .withColumnRenamed(\"churn\", \"label\") \\\n",
    "    .orderBy(\"userId\")\n",
    "label.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----------+-----------------+----------+------------------+------------------+------------------+-------+-----+\n",
      "|gender|level|num_song|num_artist|num_playlist_song|num_friend|       time_listen|         avg_songs|       avgSessTime|session|label|\n",
      "+------+-----+--------+----------+-----------------+----------+------------------+------------------+------------------+-------+-----+\n",
      "|     0|    0|    1317|       767|               25|        14|259349.89726000006|48.666666666666664| 194.9060606060606|     22|    1|\n",
      "|     1|    1|    2080|      1205|               49|        25| 443147.6018400001|104.58823529411765| 434.7392156862745|     17|    0|\n",
      "|     1|    1|     320|       223|                5|        13| 63271.01815999999| 83.33333333333333|352.27777777777777|      3|    0|\n",
      "|     1|    1|    1752|      1071|               46|        23| 364286.8624700001|163.55555555555554| 549.0606060606061|     11|    0|\n",
      "|     0|    1|     299|       215|                7|         4|59019.778759999994|              47.2|197.01666666666668|      5|    0|\n",
      "+------+-----+--------+----------+-----------------+----------+------------------+------------------+------------------+-------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "dataset = feat_gender.join(feat_level,'userID','outer') \\\n",
    "    .join(feat_song,'userID','outer') \\\n",
    "    .join(feat_artist,'userID','outer') \\\n",
    "    .join(feat_playlist,'userID','outer') \\\n",
    "    .join(feat_friends,'userID','outer') \\\n",
    "    .join(feat_listentime,'userID','outer') \\\n",
    "    .join(feat_avgsongs,'userID','outer') \\\n",
    "    .join(feat_avgtime,'userID','outer') \\\n",
    "    .join(feat_session,'userID','outer') \\\n",
    "    .join(label,'userID','outer') \\\n",
    "    .drop('userID') \\\n",
    "    .fillna(0)\n",
    "\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- num_song: long (nullable = true)\n",
      " |-- num_artist: long (nullable = true)\n",
      " |-- num_playlist_song: long (nullable = true)\n",
      " |-- num_friend: long (nullable = true)\n",
      " |-- time_listen: double (nullable = false)\n",
      " |-- avg_songs: double (nullable = false)\n",
      " |-- avgSessTime: double (nullable = false)\n",
      " |-- session: long (nullable = true)\n",
      " |-- label: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 5003|\n",
      "|    0|17275|\n",
      "+-----+-----+"
     ]
    }
   ],
   "source": [
    "dataset.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[gender: int, level: int, num_song: bigint, num_artist: bigint, num_playlist_song: bigint, num_friend: bigint, time_listen: double, avg_songs: double, avgSessTime: double, session: bigint, label: int, NumericFeatures: vector]"
     ]
    }
   ],
   "source": [
    "cols = dataset.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"NumericFeatures\")\n",
    "data = assembler.transform(dataset)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler(inputCol=\"NumericFeatures\", outputCol=\"features\", withStd=True)\n",
    "scalerModel = std_scaler.fit(data)\n",
    "data = scalerModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, estimator, paramGrid, folds=3):\n",
    "    \"\"\"\n",
    "    Fit an estimator with training data and tune it with the defined parameter grid using 3-folds cross validation\n",
    "    \"\"\"\n",
    "    crossval = CrossValidator(estimator=estimator,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(),\n",
    "                              numFolds=folds)\n",
    "    model = crossval.fit(train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate a learned model given an unseen dataset\n",
    "    \"\"\"\n",
    "    pred = model.transform(data)\n",
    "   \n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "        \n",
    "    evalMetrics = {}\n",
    "    evalMetrics[\"precision\"] = evaluator.evaluate(pred, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    evalMetrics[\"recall\"] = evaluator.evaluate(pred, {evaluator.metricName: \"weightedRecall\"})\n",
    "    evalMetrics[\"f1\"] = evaluator.evaluate(pred, {evaluator.metricName: \"f1\"})\n",
    "    evalMetrics[\"accuracy\"] = evaluator.evaluate(pred, {evaluator.metricName: \"accuracy\"})\n",
    "    \n",
    "     # Build a Spark dataframe from the metrics\n",
    "    metrics_to_display = {\n",
    "        k:round(v, 4) for k,v in evalMetrics.items() if ('confusion_matrix' not in k)\n",
    "    }\n",
    "    summary = spark.createDataFrame(pd.DataFrame([metrics_to_display], columns=metrics_to_display.keys()))\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "paramGrid_gbt = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxIter,[30])\\\n",
    "    .addGrid(gbt.maxBins, [40])\\\n",
    "    .addGrid(gbt.maxDepth,[8]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & tuning GBTClassifier mod el >\n",
      "Training time 158.85 minutes"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Training & tuning GBTClassifier model >\")\n",
    "model = train_model(train, gbt, paramGrid_gbt)\n",
    "end = time()\n",
    "print('Training time {} minutes'.format(round((end - start)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation result:\n",
      "+------+------+---------+--------+\n",
      "|    f1|recall|precision|accuracy|\n",
      "+------+------+---------+--------+\n",
      "|0.7254|0.7868|   0.7444|  0.7908|\n",
      "+------+------+---------+--------+"
     ]
    }
   ],
   "source": [
    "summary = eval_model(model, test)\n",
    "print(\"Evaluation result:\")\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same classifier and the same parameters to learn from the full dataset, the evaluation shows less promising results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bestModel.write().overwrite().save('GBTClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = GBTClassificationModel.load('GBTClassifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
